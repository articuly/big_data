{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# local - 本地运行\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"myApp\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建时指定分区数\n",
    "# 1个分区\n",
    "rdd_demo = sc.parallelize([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看rdd分区数\n",
    "rdd_demo.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2个分区\n",
    "rdd_demo = sc.parallelize([1, 2, 3, 4, 5, 6], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glom查看分区元素\n",
    "rdd_demo.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4], [5, 6]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitions 对分区进行map\n",
    "def func1(iterator):\n",
    "    yield sum(iterator)  # 只能用迭代器函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 15]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_demo.mapPartitions(func1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitionsWithIndex - 对分区map时传入分区id\n",
    "def func2(index, iteratior):\n",
    "    lists = list(iteratior)\n",
    "    print(index, '=>', lists)\n",
    "    yield (index, sum(lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6), (1, 15)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_demo.mapPartitionsWithIndex(func2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreachPartition(func) - 对分区的每一个元素进行func处理\n",
    "def func3(iter):\n",
    "    print('sum is', sum(iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_demo.foreachPartition(func3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 练习\n",
    "# 目标：计算出每个分区的a、b的和\n",
    "# [\n",
    "# [0, ('a', 140), ('b', 340)],\n",
    "# [1, ('a', 986), ('b', 462)]\n",
    "# ]\n",
    "# 使用mapPartitionsWithIndex计算每个分区，每个键的总和\n",
    "rdd = sc.parallelize([(\"a\", list(range(10))), (\"a\", list(range(5, 15))),\n",
    "                      (\"b\", list(range(10, 20))), (\"b\", list(range(15, 25))),\n",
    "                      (\"a\", list(range(25, 32))), (\"a\", list(range(20, 30))),\n",
    "                      (\"b\", list(range(63, 70))), (\"a\", list(range(50, 60)))],\n",
    "                     2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "  ('a', [5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
       "  ('b', [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       "  ('b', [15, 16, 17, 18, 19, 20, 21, 22, 23, 24])],\n",
       " [('a', [25, 26, 27, 28, 29, 30, 31]),\n",
       "  ('a', [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       "  ('b', [63, 64, 65, 66, 67, 68, 69]),\n",
       "  ('a', [50, 51, 52, 53, 54, 55, 56, 57, 58, 59])]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func4(index, iter):\n",
    "    lists = list(iter)\n",
    "    yield (index, lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('a', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "   ('a', [5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),\n",
       "   ('b', [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       "   ('b', [15, 16, 17, 18, 19, 20, 21, 22, 23, 24])]),\n",
       " (1,\n",
       "  [('a', [25, 26, 27, 28, 29, 30, 31]),\n",
       "   ('a', [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       "   ('b', [63, 64, 65, 66, 67, 68, 69]),\n",
       "   ('a', [50, 51, 52, 53, 54, 55, 56, 57, 58, 59])])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex(func4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func5(index, iter):\n",
    "    lists = list(iter)\n",
    "    print(f'{index} => {lists}')\n",
    "    res = {'a': 0, 'b': 0}\n",
    "    for i in lists:\n",
    "        res[i[0]] += sum(i[1])\n",
    "        print(i[0], '=>', sum(i[1]))\n",
    "    yield [index, ('a', res['a']), ('b', res['b'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, ('a', 140), ('b', 340)], [1, ('a', 986), ('b', 462)]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex(func5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分区下的combineByKey\n",
    "new_pair_rdd = sc.parallelize([(\"a\", [1, 2, 3]), (\"b\", [4, 5, 6]),\n",
    "                               (\"a\", [10, 20, 30]), (\"b\", [40, 50, 60]),\n",
    "                               (\"a\", [7, 8, 9]), (\"b\", [10, 11, 12]),\n",
    "                               (\"a\", [70, 80, 90]), (\"b\", [100, 110, 120])], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list(first_item):\n",
    "    print(\"first_item=\", first_item)\n",
    "    return [sum(first_item)]\n",
    "\n",
    "\n",
    "# 将后面遇到的相同的key元素添加到对应的列表\n",
    "def append(last_result, current_item):\n",
    "    print(\"args:\", last_result, current_item)\n",
    "    last_result.append(sum(current_item))\n",
    "    return last_result\n",
    "\n",
    "\n",
    "# 如果只有一个分区这个函数不会被调用\n",
    "def extend(last_partitioner, current_partinioner):\n",
    "    print(\"partitioner:\", last_partitioner, current_partinioner)\n",
    "    last_partitioner.extend(current_partinioner)\n",
    "    return last_partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd = new_pair_rdd.combineByKey(create_list, append, extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', [15, 150, 33, 330]), ('a', [6, 60, 24, 240])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark_env]",
   "language": "python",
   "name": "conda-env-spark_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
