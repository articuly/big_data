{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（10分） spark是一款什么软件，有什么特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark是一个分布式大数据处理框架，可实现批处理和流数据处理。它特点有：\n",
    "- 性能优异，在内存里进行计算，具有比Hadoop更高的Mapreduce效率，提供的功能操作也更多。\n",
    "- 支持多语言，由Scala语言编写，提供了Python、Java、R语言的API用来编写mapreduce程序。\n",
    "- 支持多种模式运行，支持单机、集群，支持Yarn、Mesos等模式，支持单个文件、Hadoop HDFS文件等多种数据源。\n",
    "- 应用场景丰富，提供了如Spark SQL、Spark Streaming、MLLIB、GraphX等组件针对不同时间跨度和场景的数据处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（10分） spark的RDD与DataFrame区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD为Resilient Distributed Dataset弹性分布式数据集，储存非结构化数据。RDD的数据由多个分区（partition）构成，可根据CPU内核数或集群节点数来设定分区数量。RDD可以在内存和磁盘存储间手动或自动切换，而持久化方式可以将数据存储到磁盘中用于后续计算的复用。\n",
    "- DataFrame是结构化数据集，储存结构化数据，它的性能比RDD更优，它类似SQL的声明式编程方式，支持SQL的语句。DataFrame就像数据库中的表，除了数据之外它还保存了数据库结构、字段类型的信息。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（10分） hadoop是一款什么软件？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hadoop是Apache基金会的一个分布式存储与计算框架。\n",
    "- 核心是HDFS和MapReduce，HDFS为海量数据提供了存储，而MapReduce为海量数据提供了计算框架。\n",
    "- 通过Hadoop框架，可以由廉价的计算机组成的服务器集群上进行可靠的存储与运算。\n",
    "- HDFS实现了一个分布式文件系，它是一个虚拟的文件系统，与普通文件系统一样，可以存储与读取文件。\n",
    "- HDFS文件在集群内分块保存，支持超大文件，支持超过一台服务器容量大小的文件。\n",
    "- 不适合大量的小文件存储，文件修改效率低。HDFS适合一次写入，多次读取的场景。\n",
    "- MapReduce是Hadoop框架下一种编程模型，Map方法是将每个文件分片由单独的机器去处理，Reduce方法将各个机器计算的结果汇总并得到最终的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（10分） MapReduce是什么过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MapReduce是一个计算框架，它给出了一种数据处理的方式，即通过Map阶段、Reduce阶段来分布式地流式处理数据。\n",
    "- Hadoop的MapReduce计算过程要保存到Hadoop的HDFS中，只适用于大数据的离线处理，对实时性要求很高的应用不适用。而Spark的MapReduce则将计算过程的结果放到内存中，大大提升计算性能，可做到实时的计算。\n",
    "- MapReduce的核心是“分而治之”策略。Map方法是将计算任务分成若干小块完成计算，Reduce将若干小块的计算结果合并。\n",
    "- MapReduce通过简单的Mapper和Reducer的抽象提供一个编程模型，可以在一个由几十台上百台的PC组成的不可靠集群上并发地，分布式地处理大量的数据集，而把并发、分布式（如机器间通信）和故障恢复等计算细节隐藏起来。\n",
    "- Mapper和Reducer对复杂的数据处理，可以分解为由多个Job（包含一个Mapper和一个Reducer）组成的有向无环图（DAG）,然后每个Mapper和Reducer放到Hadoop集群上执行，就可以得出结果。\n",
    "- MapReduce编程模型将大数据计算过程切分为map和reduce两个阶段，在map阶段为每个数据块分配一个map计算任务，然后将所有map输出的key进行合并，相同的key及其对应的value发送给同一个reduce任务去处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（10分） 什么是yarn模式？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yet another resource negotiator是对旧有Mapreduce运行机制的升级版，它改进了资源调度上的一些问题，提供了更好的资源调度机制。\n",
    "- 支持多种编程模式，通过Yarn模式，可以将spark的程序提交给hadoop运行，用这些编程模式来处理HDFS中存储的数据。\n",
    "- 它有两个核心服务：resource manager管理Hadoop的资源，node manager负责创建容器，运行提交的应用程序。\n",
    "- Yarn可以通过队列的方式，管理同时运行在yarn集群种的多个服务，可根据不同类型的应用程序压力情况，调整对应的资源使用量，实现资源弹性管理和资源隔离。\n",
    "- Yarn负责资源的管理和调度，用户可自己决定在yarn集群上运行哪种服务和应用，所以在yarn上有可能同时运行多个同类的服务和应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark_env]",
   "language": "python",
   "name": "conda-env-spark_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
