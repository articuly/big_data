{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')  # 按CPU个数的本地运行模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 只读广播变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = {'M': (1, 0, 0), 'F': (0, 1, 0), 'U': (0, 0, 1)}  # 变量\n",
    "broadcast_map = sc.broadcast(ohe)  # 广播变量\n",
    "rdd2 = sc.parallelize(['M', 'F', 'U', 'F', 'M', 'U'])\n",
    "\n",
    "\n",
    "def ohe_map_func(x, shared_map):\n",
    "    return shared_map[x]\n",
    "\n",
    "\n",
    "rdd2.map(\n",
    "    lambda x: ohe_map_func(x, broadcast_map.value)).collect()  # value属性为访问广播变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_map.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)\n",
    "\n",
    "\n",
    "def split_line(line):\n",
    "    if len(line) == 0:\n",
    "        accum.add(1)  # 空则累加1\n",
    "    return 1  # 一行则有1\n",
    "\n",
    "\n",
    "rdd3 = sc.textFile(\n",
    "    'file:///D:/Projects/python_projects/big_data/shakespeare_all.txt')\n",
    "tot_lines = rdd3.map(split_line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_lines = accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169442\n"
     ]
    }
   ],
   "source": [
    "print(tot_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28854\n"
     ]
    }
   ],
   "source": [
    "print(empty_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "bcast_dataset = sc.broadcast(load_iris())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import AccumulatorParam\n",
    "\n",
    "\n",
    "class ErrorAccumulator(AccumulatorParam):\n",
    "\n",
    "    def zero(self, initialList):  # 初始化方法\n",
    "        return initialList\n",
    "\n",
    "    def addInPlace(self, v1, v2):  # add方法，组合一个元组和一个列表，但不分先后\n",
    "        if not isinstance(v1, list):\n",
    "            v1 = [v1]\n",
    "        if not isinstance(v2, list):\n",
    "            v2 = [v2]\n",
    "        return v1 + v2\n",
    "\n",
    "\n",
    "errAccum = sc.accumulator([], ErrorAccumulator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classifier(clf, dataset):\n",
    "    clf_name = clf.__class__.__name__\n",
    "    X = dataset.value.data\n",
    "    y = dataset.value.target\n",
    "    try:\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict(X)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        return [(clf_name, acc)]\n",
    "    except Exception as e:\n",
    "        errAccum.add((clf_name, str(e)))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DummyClassifier', 0.3333333333333333), ('SGDClassifier', 0.92)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "classifiers=[\n",
    "    DummyClassifier(strategy='most_frequent'),\n",
    "    SGDClassifier(),\n",
    "    PCA(),\n",
    "    MDS()\n",
    "]\n",
    "\n",
    "print(sc.parallelize(classifiers).flatMap(lambda clf: apply_classifier(clf, bcast_dataset)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The errors are: [('MDS', \"'MDS' object has no attribute 'predict'\"), ('PCA', \"'PCA' object has no attribute 'predict'\")]\n"
     ]
    }
   ],
   "source": [
    "print('The errors are:', errAccum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LogisticRegression'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark_env]",
   "language": "python",
   "name": "conda-env-spark_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
